{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a698d26",
   "metadata": {},
   "source": [
    "# PySpark - Project Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "264c5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import udf, StringType\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69f5fed",
   "metadata": {},
   "source": [
    "# Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033a5123",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 22:31:27 WARN Utils: Your hostname, muhammad-Vm resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/04/25 22:31:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/25 22:31:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('project_tweets').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb78f71",
   "metadata": {},
   "source": [
    "# Read the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a61042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the ProjectTweets into Hadoop in the named folder 'CA2'\n",
    "\n",
    "data = spark.read.csv('hdfs://localhost:9000/CA2/ProjectTweets.csv', header=True, inferSchema =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "507a5d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 0: integer (nullable = true)\n",
      " |-- 1467810369: long (nullable = true)\n",
      " |-- Mon Apr 06 22:19:45 PDT 2009: string (nullable = true)\n",
      " |-- NO_QUERY: string (nullable = true)\n",
      " |-- _TheSpecialOne_: string (nullable = true)\n",
      " |-- @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the structure of schema\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c534db",
   "metadata": {},
   "source": [
    "# Rename Target Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b08643",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_columns = data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50967eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = Row(*old_columns)(*old_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebba0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.createDataFrame([new_row], old_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c0c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = new_df.union(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6cfa24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomeie as colunas\n",
    "new_column_names = [\"index\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "df_renamed = combined_df.toDF(*new_column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88ec4ba1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+--------+---------------+--------------------+\n",
      "|index|       ids|                date|    flag|           user|                text|\n",
      "+-----+----------+--------------------+--------+---------------+--------------------+\n",
      "|    0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|    1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|    2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "+-----+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar o DataFrame com as colunas renomeadas\n",
    "df_renamed.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5856ca84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: string (nullable = true)\n",
      " |-- ids: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_renamed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ec2ec0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas: 1600000\n",
      "Número de colunas: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Obtém o número de linhas\n",
    "num_rows = df_renamed.count()\n",
    "print(f\"Número de linhas: {num_rows}\")\n",
    "\n",
    "# Obtém o número de colunas\n",
    "num_columns = len(df_renamed.columns)\n",
    "print(f\"Número de colunas: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "156bd476",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 22:32:30 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------------------+--------+-------------------+--------------------+\n",
      "|summary|             index|                 ids|                date|    flag|               user|                text|\n",
      "+-------+------------------+--------------------+--------------------+--------+-------------------+--------------------+\n",
      "|  count|           1600000|             1600000|             1600000| 1600000|            1600000|             1600000|\n",
      "|   mean|          799999.5|1.9988175522956276E9|                NULL|    NULL|4.325887521835714E9|                NULL|\n",
      "| stddev|461880.35968924535|1.9357607362269327E8|                NULL|    NULL|5.16273321845489E10|                NULL|\n",
      "|    min|                 0|          1467810369|Fri Apr 17 20:30:...|NO_QUERY|       000catnap000|                 ...|\n",
      "|    25%|          400000.0|       1.956901697E9|                NULL|    NULL|            32508.0|                NULL|\n",
      "|    50%|          800000.0|       2.002093535E9|                NULL|    NULL|           130587.0|                NULL|\n",
      "|    75%|         1200038.0|       2.177048867E9|                NULL|    NULL|          1100101.0|                NULL|\n",
      "|    max|            999999|          2329205794|Wed May 27 07:27:...|NO_QUERY|         zzzzeus111|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|\n",
      "+-------+------------------+--------------------+--------------------+--------+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_renamed.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b047be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "spark = spark.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e4e6e70",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `ids, date` cannot be resolved. Did you mean one of the following? [`date`, `ids`, `index`, `flag`, `text`].;\n'Project ['ids, date, user#54, text#55]\n+- Project [index#50, ids#51, to_timestamp(date#52, Some(EEE MMM dd HH:mm:ss zzz yyyy), TimestampType, Some(Europe/Dublin), false) AS date#928, flag#53, user#54, text#55]\n   +- Project [0#29 AS index#50, 1467810369#30 AS ids#51, Mon Apr 06 22:19:45 PDT 2009#31 AS date#52, NO_QUERY#32 AS flag#53, _TheSpecialOne_#33 AS user#54, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D#34 AS text#55]\n      +- Union false, false\n         :- LogicalRDD [0#29, 1467810369#30, Mon Apr 06 22:19:45 PDT 2009#31, NO_QUERY#32, _TheSpecialOne_#33, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D#34], false\n         +- Project [cast(0#17 as string) AS 0#41, cast(1467810369#18L as string) AS 1467810369#42, Mon Apr 06 22:19:45 PDT 2009#19, NO_QUERY#20, _TheSpecialOne_#21, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D#22]\n            +- Relation [0#17,1467810369#18L,Mon Apr 06 22:19:45 PDT 2009#19,NO_QUERY#20,_TheSpecialOne_#21,@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D#22] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6384/1557992598.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ids, date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3225\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m         \"\"\"\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `ids, date` cannot be resolved. Did you mean one of the following? [`date`, `ids`, `index`, `flag`, `text`].;\n'Project ['ids, date, user#54, text#55]\n+- Project [index#50, ids#51, to_timestamp(date#52, Some(EEE MMM dd HH:mm:ss zzz yyyy), TimestampType, Some(Europe/Dublin), false) AS date#928, flag#53, user#54, text#55]\n   +- Project [0#29 AS index#50, 1467810369#30 AS ids#51, Mon Apr 06 22:19:45 PDT 2009#31 AS date#52, NO_QUERY#32 AS flag#53, _TheSpecialOne_#33 AS user#54, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D#34 AS text#55]\n      +- Union false, false\n         :- LogicalRDD [0#29, 1467810369#30, Mon Apr 06 22:19:45 PDT 2009#31, NO_QUERY#32, _TheSpecialOne_#33, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D#34], false\n         +- Project [cast(0#17 as string) AS 0#41, cast(1467810369#18L as string) AS 1467810369#42, Mon Apr 06 22:19:45 PDT 2009#19, NO_QUERY#20, _TheSpecialOne_#21, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D#22]\n            +- Relation [0#17,1467810369#18L,Mon Apr 06 22:19:45 PDT 2009#19,NO_QUERY#20,_TheSpecialOne_#21,@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D#22] csv\n"
     ]
    }
   ],
   "source": [
    "tweets = tweets.select(\"ids, date\", \"user\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f66e5fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df_renamed.withColumn(\"date\", to_timestamp(df_renamed[\"date\"], \"EEE MMM dd HH:mm:ss zzz yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "939ebee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------------------+--------+---------------+--------------------+\n",
      "|index|       ids|               date|    flag|           user|                text|\n",
      "+-----+----------+-------------------+--------+---------------+--------------------+\n",
      "|    0|1467810369|2009-04-07 06:19:45|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|    1|1467810672|2009-04-07 06:19:49|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|    2|1467810917|2009-04-07 06:19:53|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|    3|1467811184|2009-04-07 06:19:57|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|    4|1467811193|2009-04-07 06:19:57|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+-----+----------+-------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:======================================>                   (2 + 1) / 3]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tweets.show(truncate=True, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e02fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
